{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Titanic models\n",
    "In this notebook, we will compare the three models we've created for the Titanic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, boto3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Since we will be making predictions in this file, we will load the test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('x_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "pycaret_model = load_model('pycaret_best_pipeline')\n",
    "    \n",
    "with open('knn_model.pkl', 'rb') as file:\n",
    "    knn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saved the predictions from the AWS model, we will need to load them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='titanic-bucket-mj'\n",
    "prefix='titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/titanic_test.csv.out\".format(prefix))\n",
    "aws_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_label(x):\n",
    "    threshold = 0.7\n",
    "    return 1 if x > threshold else 0\n",
    "\n",
    "aws_predicted = aws_predicted['survived'].apply(convert_to_label)\n",
    "\n",
    "aws_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Here, we use each model individually to predict the results it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "knn_predicted = knn_model.predict(x_test)\n",
    "print(knn_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "pycaret_predicted = pycaret_model.predict(x_test)\n",
    "print(pycaret_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "In this section, we will use various metrics to compare the predictions made by our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_TN, p_FP, p_FN, p_TP = confusion_matrix(y_test, pycaret_predicted).ravel()\n",
    "a_TN, a_FP, a_FN, a_TP = confusion_matrix(y_test, aws_predicted).ravel()\n",
    "k_TN, k_FP, k_FN, k_TP = confusion_matrix(y_test, knn_predicted).ravel()\n",
    "\n",
    "data_classification = {\n",
    "    'Model': ['pycaret', 'aws', 'knn'],\n",
    "    'TN': [p_TN, a_TN, k_TN],\n",
    "    'FP': [p_FP, a_FP, k_FP],\n",
    "    'FN': [p_FN, a_FN, k_FN],\n",
    "    'TP': [p_TP, a_TP, k_TP]\n",
    "}\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data_classification)\n",
    "\n",
    "models = confusion_matrix_df['Model']\n",
    "confusion_matrices = [\n",
    "    np.array([[row['TN'], row['FP']],\n",
    "              [row['FN'], row['TP']]]) for _, row in confusion_matrix_df.iterrows()\n",
    "]\n",
    "\n",
    "# Create a heatmap for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), constrained_layout=True)\n",
    "\n",
    "for ax, matrix, model in zip(axes, confusion_matrices, models):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(f'{model} Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xticklabels(['Died', 'Survived'])\n",
    "    ax.set_yticklabels(['Died', 'Survived'])\n",
    "\n",
    "#since we have some font-errors that won't be resolved that easily within sagemaker, so we save the matrices as an image\n",
    "plt.savefig('confusion_matrices.png')\n",
    "print(\"Plot saved as 'confusion_matrices.png'.\")\n",
    "    \n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to an unexpected error with fonts in SageMaker, we will display an image of the results instead. As seen, it is relatively easy for most models to predict when a passenger died, but determining survival is much more challenging. This could be attributed to the imbalance in the 'survived' column, with fewer 'survived' values compared to 'died' values.\n",
    "\n",
    "![confusion matrices](images/confusion_matrices.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Negative Predictive Value</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <th>Validation AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pycaret</td>\n",
       "      <td>0.879929</td>\n",
       "      <td>0.759241</td>\n",
       "      <td>0.936632</td>\n",
       "      <td>0.849153</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.063368</td>\n",
       "      <td>0.240759</td>\n",
       "      <td>0.150847</td>\n",
       "      <td>0.847937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aws</td>\n",
       "      <td>0.864172</td>\n",
       "      <td>0.637747</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>0.910518</td>\n",
       "      <td>0.850802</td>\n",
       "      <td>0.029447</td>\n",
       "      <td>0.362253</td>\n",
       "      <td>0.089482</td>\n",
       "      <td>0.804150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.864826</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>0.918328</td>\n",
       "      <td>0.812028</td>\n",
       "      <td>0.886983</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>0.249049</td>\n",
       "      <td>0.187972</td>\n",
       "      <td>0.834639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Accuracy  Sensitivity  Specificity  Precision  \\\n",
       "0  pycaret  0.879929     0.759241     0.936632   0.849153   \n",
       "1      aws  0.864172     0.637747     0.970553   0.910518   \n",
       "2      knn  0.864826     0.750951     0.918328   0.812028   \n",
       "\n",
       "   Negative Predictive Value  False Positive Rate  False Negative Rate  \\\n",
       "0                   0.892244             0.063368             0.240759   \n",
       "1                   0.850802             0.029447             0.362253   \n",
       "2                   0.886983             0.081672             0.249049   \n",
       "\n",
       "   False Discovery Rate  Validation AUC  \n",
       "0              0.150847        0.847937  \n",
       "1              0.089482        0.804150  \n",
       "2              0.187972        0.834639  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_metrics(row):\n",
    "    TN, FP, FN, TP = row['TN'], row['FP'], row['FN'], row['TP']\n",
    "    total = TN + FP + FN + TP\n",
    "    \n",
    "    accuracy = (TP + TN) / total\n",
    "    sensitivity = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    specificity = TN / (TN + FP) if TN + FP > 0 else 0\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    npv = TN / (TN + FN) if TN + FN > 0 else 0\n",
    "    fpr = FP / (FP + TN) if FP + TN > 0 else 0\n",
    "    fnr = FN / (FN + TP) if FN + TP > 0 else 0\n",
    "    fdr = FP / (FP + TP) if FP + TP > 0 else 0\n",
    "    \n",
    "    return pd.Series([accuracy, sensitivity, specificity, precision, npv, fpr, fnr, fdr])\n",
    "\n",
    "metrics_df = confusion_matrix_df.apply(calculate_metrics, axis=1)\n",
    "metrics_df.columns = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision',\n",
    "                      'Negative Predictive Value', 'False Positive Rate',\n",
    "                      'False Negative Rate', 'False Discovery Rate']\n",
    "\n",
    "# Has to be added afterwards since it uses the predicted values\n",
    "validation_auc = [\n",
    "    roc_auc_score(y_test, pycaret_predicted),\n",
    "    roc_auc_score(y_test, aws_predicted),\n",
    "    roc_auc_score(y_test, knn_predicted)\n",
    "]\n",
    "\n",
    "metrics_df['Validation AUC'] = validation_auc\n",
    "\n",
    "# We add the Model-column since that makes it clearer which metric responds to which model\n",
    "metrics_df.insert(0, 'Model', confusion_matrix_df['Model'])\n",
    "\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that all models are highly accurate. However, while the PyCaret and KNN models are better at correctly predicting positives, the AWS/XGBoost model performs significantly better at predicting negatives compared to the others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
